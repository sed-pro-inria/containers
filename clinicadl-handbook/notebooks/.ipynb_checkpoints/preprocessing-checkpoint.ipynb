{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbea8d2c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Uncomment the next lines if running in Google Colab\n",
    "# !pip install clinicadl==0.2.1\n",
    "# !/bin/bash -c \"$(curl -k https://aramislab.paris.inria.fr/files/software/scripts/install_conda_ants.sh)\"\n",
    "# # from os import environ\n",
    "# # environ['ANTSPATH']=\"/usr/local/bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc11e29",
   "metadata": {},
   "source": [
    "# Prepare your neuroimaging data\n",
    "\n",
    "Different steps to perform before training your model or performing classification. In this notebook, we will see how to:\n",
    "\n",
    "1. **Organize** your neuroimaging data.\n",
    "2. **Preprocess** of your neuroimaging data.\n",
    "3. Check the preprocessing **quality check**.\n",
    "4. **Extract tensors** from your preprocessed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26216c3c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Organization of neuroimaging data: the Brain Imaging Data Structure (BIDS)\n",
    "\n",
    "Before processing your neuroimaging data, several steps may be needed. These\n",
    "steps can include converting the images to a format readable by neuroimaging\n",
    "software tools (e.g. converting to NIfTI) and organizing your files in a\n",
    "specific way. Several tools will require that your clinical and imaging data\n",
    "follow the **Brain Imaging Data Structure (BIDS)** [(Gorgolewski et al.,\n",
    "2016)](https://doi.org/10.1038/sdata.2016.44). The BIDS standard is based on a\n",
    "file hierarchy rather than on a database management system, thus facilitating\n",
    "its deployment. Thanks to its clear and simple way to describe neuroimaging\n",
    "and behavioral data, it has been easily adopted by the neuroimaging community.\n",
    "Organizing a dataset following the BIDS hierarchy simplifies the execution of\n",
    "neuroimaging software tools. \n",
    "\n",
    "Here is a general overview of the BIDS structure. If you need more details,\n",
    "please check the\n",
    "[documentation](https://bids-specification.readthedocs.io/en/latest/) on the\n",
    "[website](http://bids.neuroimaging.io/).\n",
    "\n",
    "<pre>\n",
    "BIDS_Dataset/\n",
    "├── participants.tsv\n",
    "├── sub-CLNC01/\n",
    "│   │   ├── ses-M00/\n",
    "│   │   │   └── anat/\n",
    "│   │   │       └── <b>sub-CLNC01_ses-M00_T1w.nii.gz</b>\n",
    "│   │   └── sub-CLNC01_sessions.tsv\n",
    "├── sub-CLNC02/\n",
    "│   │   ├── ses-M00/\n",
    "│   │   │   └── anat/\n",
    "│   │   │       └── <b>sub-CLNC02_ses-M00_T1w.nii.gz</b>\n",
    "│   │   └── sub-CLNC02_sessions.tsv\n",
    "└──  ...\n",
    "</pre>\n",
    "\n",
    "\n",
    "The OASIS dataset contains imaging data in ANALYZE format and does not provide\n",
    "a BIDS version of the data. To solve this issue, [Clinica provides a\n",
    "converter](https://aramislab.paris.inria.fr/clinica/docs/public/latest/Converters/OASIS2BIDS/)\n",
    "to automatically convert ANALYZE files into NIfTI following the BIDS standard.\n",
    "\n",
    "A command line instruction is enough to get the data in BIDS format:\n",
    "\n",
    "```bash\n",
    "clinica convert oasis-to-bids <dataset_directory> <clinical_data_directory> <bids_directory>\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "  - `dataset_directory` is the path to the original OASIS images' directory;\n",
    "  - `clinical_data_directory` is the path to the directory containing the `oasis_cross-sectional.csv` file;\n",
    "  - `bids_directory` is the path to the output directory, where the BIDS-converted version of OASIS will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870e2543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the example dataset of 4 images\n",
    "!curl -k https://aramislab.paris.inria.fr/files/data/databases/tuto/OasisDatabase.tar.gz -o OasisDatabase.tar.gz\n",
    "!tar xf OasisDatabase.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263fc582",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Convert the example dataset to BIDS\n",
    "!clinica convert oasis-to-bids OasisDatabase/RawData OasisDatabase/ClinicalData OasisBids_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e43b8e5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<a id='preprocessing:t1-linear'></a>\n",
    "## Image preprocessing with the `t1-linear` pipeline\n",
    "\n",
    "Although convolutional neural networks (CNNs) have the potential to extract low-to-high level features from raw images, a proper image preprocessing procedure is fundamental to ensure a good classification performance (in particular for Alzheimer's disease (AD) classification where datasets are relatively small).\n",
    "\n",
    "In the context of deep learning-based classification, image preprocessing procedures often include:\n",
    "\n",
    "- **Bias field correction:** MR images can be corrupted by a low frequency and smooth signal caused by magnetic field inhomogeneities. This bias field induces variations in the intensity of the same tissue in different locations of the image, which deteriorates the performance of image analysis algorithms such as registration.\n",
    "\n",
    "- **Image registration:** Medical image registration consists of spatially aligning two or more images, either globally (rigid and affine registration) or locally (non-rigid registration), so that voxels in corresponding positions contain comparable information.\n",
    "\n",
    "Finally, a **Cropping** of the registered images can be performed to remove the background and to reduce the computing power required when training deep learning models.\n",
    "\n",
    "For this tutorial, we propose a \"minimal preprocessing\" (as described in [(Wen et al., 2020)](https://doi.org/10.1016/j.media.2020.101694)) implemented in the [`t1-linear` pipeline](http://www.clinica.run/doc/Pipelines/T1_Linear/) using the [ANTs](http://stnava.github.io/ANTs/) software package [(Avants et al., 2014)](https://doi.org/10.3389/fninf.2014.00044). This preprocessing includes:\n",
    "\n",
    "- **Bias field correction** using the N4ITK method [(Tustison et al., 2010)](https://doi.org/10.1109/TMI.2010.2046908).\n",
    "\n",
    "- **Affine registration** to the MNI152NLin2009cSym template (Fonov et al., [2011](https://doi.org/10.1016/j.neuroimage.2010.07.033), [2009](https://doi.org/10.1016/S1053-8119(09)70884-5) ) in MNI space with the SyN algorithm [(Avants et al., 2008)](https://doi.org/10.1016/j.media.2007.06.004).\n",
    "\n",
    "- **Cropping** resulting in final images of size 169×208×179 with 1 mm3 isotropic voxels.\n",
    "\n",
    "If you run this notebook locally, please check that ANTs is correctly installed. If it is not the case, uncomment the three last lines of the first cell and run it.\n",
    "\n",
    "\n",
    "These steps can be run with this simple command line:\n",
    "```Text\n",
    "clinica run t1-linear <bids_directory> <caps_directory>\n",
    "```\n",
    "where:\n",
    "\n",
    "- `bids_directory` is the input folder containing the dataset in a [BIDS](http://www.clinica.run/doc/BIDS/) hierarchy,\n",
    "- `caps_directory` is the output folder containing the results in a [CAPS](http://www.clinica.run/doc/CAPS/) hierarchy.\n",
    "%%[markdown]\n",
    "```{warning}\n",
    "The following command can take some time to execute, depending on the\n",
    "configuration of your host machine. Running in a classical **Colab** instance\n",
    "can take up to 30 min.\n",
    "\n",
    "We will increase a little bit the computation capacity using 2 cores with the\n",
    "`--n_procs 2` flag. Since there are 4 images, you can set `--n_procs 4` if\n",
    "your computer can handle this.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e0e115",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!clinica run t1-linear ./OasisBids_example ./OasisCaps_example --n_procs 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8c13d0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Once the pipeline has been run, the necessary outputs for the next steps are saved using a specific suffix: \n",
    "   `_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eead6c5b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "```{warning}\n",
    "The registration algorithm provided by ANTs exposes some reproducibility issues\n",
    "when running in different environments. The outputs are \"visually\" very close\n",
    "but not exactly the same. For further information and some clues on how to\n",
    "reduce the variability please read this\n",
    "[page](https://github.com/ANTsX/ANTs/wiki/antsRegistration-reproducibility-issues).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e22b083",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "For example, processed images from our dataset are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8aa8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "\n",
    "suffix = '_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz'\n",
    "\n",
    "sub1 = 'OasisCaps_example/subjects/sub-OASIS10016/ses-M00/t1_linear/sub-OASIS10016_ses-M00' + suffix \n",
    "sub2 = 'OasisCaps_example/subjects/sub-OASIS10109/ses-M00/t1_linear/sub-OASIS10109_ses-M00' + suffix\n",
    "sub3 = 'OasisCaps_example/subjects/sub-OASIS10304/ses-M00/t1_linear/sub-OASIS10304_ses-M00' + suffix\n",
    "sub4 = 'OasisCaps_example/subjects/sub-OASIS10363/ses-M00/t1_linear/sub-OASIS10363_ses-M00' + suffix\n",
    "\n",
    "plotting.plot_anat(sub1, title=\"sub-OASIS10016\")\n",
    "plotting.plot_anat(sub2, title=\"sub-OASIS10109\")\n",
    "plotting.plot_anat(sub3, title=\"sub-OASIS10304\")\n",
    "plotting.plot_anat(sub4, title=\"sub-OASIS10363\")\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cf996a",
   "metadata": {},
   "source": [
    "From the visualization above, we can see that the last two images have some\n",
    "missing skin voxels on top of the brain i.e. these images are slightly cropped.\n",
    "Besides, we did not compare them to the [MNI152NLin2009cSym\n",
    "template](https://bids-specification.readthedocs.io/en/stable/99-appendices/08-coordinate-systems.html)\n",
    "to evaluate the quality of the registration.\n",
    "\n",
    "OASIS-1 dataset contains 416 images so quality check of the whole dataset can be\n",
    "very time consuming. The next section gives you some ideas on how to keep only\n",
    "images correctly preprocessed, when running in a large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aacdaa7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Quality check of your preprocessed data\n",
    "\n",
    "To automatically assess the quality of the preprocessing, we propose to use a\n",
    "pretrained network which learnt to classify images that are adequately\n",
    "registered to a template from others for which the registration failed. This\n",
    "procedure is adaptated from [(Fonov et al,\n",
    "2018)](https://www.biorxiv.org/content/10.1101/303487v1), using their\n",
    "pretrained models. The original code of [(Fonov et al,\n",
    "2018)](https://www.biorxiv.org/content/10.1101/303487v1) can be found on\n",
    "[GitHub](https://github.com/vfonov/deep-qc).\n",
    "\n",
    "The quality check can be run with the following command line:\n",
    "```\n",
    "!clinicadl quality-check <preprocessing> <caps_directory> <output_path>\n",
    "```\n",
    "where:\n",
    "\n",
    "- `preprocessing` corresponds to the preprocessing pipeline whose outputs will be checked (`t1-linear`),\n",
    "- `caps_directory` is the folder containing the results of the `t1-linear` pipeline in a [CAPS](http://www.clinica.run/doc/CAPS/Introduction/) hierarchy,\n",
    "- `output_path` is the path to the output TSV file containing QC results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2909f0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!clinicadl extract image OasisCaps_example \"t1-linear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933996a9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!clinicadl quality-check t1-linear OasisCaps_example QC_result.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cf5033",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "After execution of the quality check procedure, the `QC_result.tsv` file will\n",
    "look like this:\n",
    "\n",
    "| participant_id | session_id | pass_probability   | pass  |\n",
    "|----------------| -----------|--------------------|-------|\n",
    "| sub-OASIS10016 | ses-M00    | 0.9936990737915039 | True  |\n",
    "| sub-OASIS10109 | ses-M00    | 0.9772214889526367 | True  |\n",
    "| sub-OASIS10363 | ses-M00    | 0.7292165160179138 | True  |\n",
    "| sub-OASIS10304 | ses-M00    | 0.1549495905637741 | <font color=\"red\">False</font> |\n",
    "\n",
    "Based on this TSV file, participant `OASIS10304` should be discarded for the\n",
    "rest of your anlysis. If you compare its registration with [MNI152NLin2009cSym\n",
    "template](https://bids-specification.readthedocs.io/en/stable/99-appendices/08-coordinate-systems.html),\n",
    "you will see that temporal regions are misaligned as well as occipital regions\n",
    "and cerebellum leading to this low probabilty value. ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749262f7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "##  Tensor extraction with the `deeplearning-prepare-data` pipeline\n",
    "\n",
    "Once the dataset has been preprocessed, we need to obtain files suited for the\n",
    "training phase.  This task can be performed using the [Clinica\n",
    "`deeplearning-prepare-data`\n",
    "pipeline](http://www.clinica.run/doc/Pipelines/DeepLearning_PrepareData/).\n",
    "\n",
    "This pipeline prepares images generated by Clinica to be used with the PyTorch\n",
    "deep learning library [(Paszke et al.,\n",
    "2019)](https://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library).\n",
    "Three types of tensors are proposed: 3D images, 3D patches or 2D slices.\n",
    "\n",
    "This pipeline selects the preprocessed images, extract the \"tensors\", and write\n",
    "them as output files for the entire images, for each slice or for each patch.\n",
    "\n",
    "You simply need to type the following command line:\n",
    "\n",
    "```bash\n",
    "clinica run deeplearning-prepare-data <caps_directory> <tensor_format>\n",
    "```\n",
    "where:\n",
    "\n",
    "- `caps_directory` is the folder containing the results of the [`t1-linear`\n",
    "pipeline](#Preprocess-raw-images-with-t1-linear-pipeline) and the output of\n",
    "the present command, both in a CAPS hierarchy.\n",
    "- `tensor_format` is the format of the extracted tensors. You can choose\n",
    "between `image` to convert to PyTorch tensor the whole 3D image, `patch` to\n",
    "extract 3D patches and `slice` to extract 2D slices from the image.\n",
    "\n",
    "Output files are stored into a new folder (inside the CAPS) and follows a struture like this:\n",
    "\n",
    "```text\n",
    "deeplearning_prepare_data\n",
    "├── image_based\n",
    "│   └── t1_linear\n",
    "│       └── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.pt\n",
    "├── patch_based\n",
    "│   └── t1_linear\n",
    "│       ├── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_patchsize-50_stride-50_patch-0_T1w.pt\n",
    "│       ├── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_patchsize-50_stride-50_patch-1_T1w.pt\n",
    "│       ├── ...\n",
    "│       └── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_patchsize-50_stride-50_patch-N_T1w.pt\n",
    "└── slice_based\n",
    "    └── t1_linear\n",
    "        ├── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_axis-axi_channel-rgb_slice-0_T1w.pt\n",
    "        ├── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_axis-axi_channel-rgb_slice-1_T1w.pt\n",
    "        ├── ...\n",
    "        ├── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_axis-axi_channel-rgb_slice-N_T1w.pt\n",
    "```\n",
    "\n",
    "In short, there is a folder for each feature (**image, slice or patch**) and\n",
    "inside the numbered tensor files with the corresponding feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a778a89",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** You can choose to only extract the tensors for the whole images\n",
    "*(`clinica run deeplearning-prepare-data <caps_directory> image` ) and continue\n",
    "*working with one single file per subject/session. \n",
    "    \n",
    "The package `clinicadl` is able to extract patches or slices _on-the-fly_ (from\n",
    "one single file) when running training or inference tasks. The downside of this\n",
    "approach is that, depending on the size of your dataset, you have to make sure\n",
    "that you have enough memory ressources in your GPU card to host the full\n",
    "images/tensors for all your data. \n",
    "\n",
    "If the memory size of the GPU card you use is too small, we suggest you to\n",
    "extract the patches and/or the slices using the proper `tensor_format` option of\n",
    "the command described above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da81bea6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "(If you failed to obtain the preprocessing using the `t1-linear` pipeline,\n",
    "please uncomment the next cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ca36c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!curl -k https://aramislab.paris.inria.fr/files/data/databases/tuto/OasisCaps1.tar.gz -o OasisCaps1.tar.gz\n",
    "!tar xf OasisCaps1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa56564",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "To perform the feature extraction for our dataset, run the following cell:     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e69131",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!clinica run deeplearning-prepare-data ./OasisCaps_example t1-linear image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33aac12",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "At the end of this command, a new directory named `deeplearning_prepare_data` is\n",
    "created inside each subject/session of the CAPS structure. We can easily verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c59748",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree -L 3 ./OasisCaps_example/subjects/sub-OASIS10*/ses-M00/deeplearning_prepare_data/"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
