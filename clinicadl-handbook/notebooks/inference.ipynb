{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0407725",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Uncomment this cell if running in Google Colab\n",
    "!pip install clinicadl==1.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234ad0b3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Perfom classification using pretrained models\n",
    "\n",
    "<SCRIPT SRC='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></SCRIPT>\n",
    "<SCRIPT>MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}})</SCRIPT> \n",
    "\n",
    "This notebook shows how to perform classification on preprocessed data using pretrained models described in ([Wen et al, 2020](https://www.sciencedirect.com/science/article/abs/pii/S1361841520300591)).\n",
    "\n",
    "## Structure of the pretrained models\n",
    "\n",
    "All the pretrained model folders are organized as follows:\n",
    "<pre>\n",
    "<b>results</b>\n",
    "├── commandline.json\n",
    "├── <b>fold-0</b>\n",
    "├── ...\n",
    "└── <b>fold-4</b>\n",
    "    ├── <b>models</b>\n",
    "    │      └── <b>best_balanced_accuracy</b>\n",
    "    │          └── model_best.pth.tar\n",
    "    └── <b>cnn_classification</b>\n",
    "           └── <b>best_balanced_accuracy</b>\n",
    "               └── validation_{patch|roi|slice}_level_prediction.tsv\n",
    "</pre>\n",
    "This file system is a part of the output of `clinicadl train` and `clinicadl classify` relies on three files:\n",
    "<ul>\n",
    "    <li> <code>commandline.json</code> contains all the options that were entered for training (type of input, architecture, preprocessing...).</li>\n",
    "    <li> <code>model_best.pth.tar</code> corresponds to the model selected when the best validation balanced accuracy was obtained.</li>\n",
    "    <li> <code>validation_{patch|roi|slice}_level_prediction.tsv</code> is specific to patch, roi and slice frameworks and is necessary to perform <b>soft-voting</b>  and find the label at the image level in unbiased way. Weighting the patches based on their performance of input data would bias the result as the classification framework would exploit knowledge of the test data.</li>\n",
    "</ul>\n",
    "\n",
    "<div class=\"admonition tip\" name=\"html-admonition\" style=\"background: lightgreen; padding: 10px\">\n",
    "<p class=\"title\">Tip</p>\n",
    "    <p> You can use your own previuolsy trained model (if you have used PyTorch\n",
    "    for that). Indeed, PyTorch stores model weights in a file with extension\n",
    "    <i>pth.tar</i>. You can place this file into the <i>models</i> folder and\n",
    "    try to follow the same structure that is described above. You also need to\n",
    "    fill a <i>commandline.json</i> file with all the parameters used during\n",
    "    the training (see <a\n",
    "    href=\"https://clinicadl.readthedocs.io/en/latest/Train/Introduction/#outputs\">ClinicaDL\n",
    "    documentation</a>) for further info.</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<p class=\"title\">Soft voting</p>\n",
    "   For classification tasks that take as input a part of the MRI volume\n",
    "   (<i>patch, roi or slice</i>), an ensemble operation is needed to obtain the\n",
    "   label at the image level.</p>\n",
    "   <p>For example, size and stride of 50 voxels on linear preprocessing leads to\n",
    "   the classification of 36 patches, but they are not all equally meaningful.\n",
    "   Patches that are in the corners of the image are mainly composed of background\n",
    "   and skull and may be misleading, whereas patches within the brain may be more\n",
    "   useful.</p>\n",
    "   <img src=\"../images/patches.png\">\n",
    "   <p>Then the image-level probability of AD <i>p<sup>AD</sup></i> will be:</p>\n",
    "   $$ p^{AD} = {\\sum_{i=0}^{35} bacc_i * p_i^{AD}}. $$\n",
    "   where:<ul>\n",
    "   <li> <i>p<sub>i</sub><sup>AD</sup></i> is the probability of AD for patch <i>i</i></li>\n",
    "   <li> <i>bacc<sub>i</sub></i> is the validation balanced accuracy for patch <i>i</i></li>\n",
    "   </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797698a7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Download the pretrained models\n",
    "\n",
    "<div class=\"admonition warning\" name=\"html-admonition\" style=\"background: lightgreen; padding: 10px\">\n",
    "<p class=\"title\">Warning</p>\n",
    "For the sake of the demonstration, this tutorial uses truncated versions of\n",
    "the models, containing only the first fold.\n",
    "</div>\n",
    "\n",
    "In this notebook, we propose to use 4 specific models , all of them where trained to predict the classification task AD vs CN. (The experiment corresponding to the pretrained model in eTable 4 of the paper mentioned above is shown below):\n",
    "\n",
    "1. **3D image-level model**, pretrained with the baseline data and initialized with an autoencoder (_cf._ exp. 3).\n",
    "2. **3D ROI-based model**, pretrained with the baseline data and initialized with an autoencoder (_cf._ exp. 8).\n",
    "3. **3D patch-level model**, multi-cnn, pretrained with the baseline data and initialized with an autoencoder (_cf._ exp. 14).\n",
    "4. **2D slice-level model**, pretrained with the baseline data and initialized with an autoencoder (_cf._ exp. 18).\n",
    "\n",
    "Commands in the next code cell will automatically download and uncompress these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff116595",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Download here the pretrained models stored online\n",
    "# Model 1\n",
    "!curl -k https://aramislab.paris.inria.fr/clinicadl/files/models/v0.2.0/model_exp3_splits_1.tar.gz  -o model_exp3_splits_1.tar.gz\n",
    "!tar xf model_exp3_splits_1.tar.gz\n",
    "\n",
    "# Model 2\n",
    "!curl -k https://aramislab.paris.inria.fr/clinicadl/files/models/v0.2.0/model_exp8_splits_1.tar.gz  -o model_exp8_splits_1.tar.gz\n",
    "!tar xf model_exp8_splits_1.tar.gz\n",
    "\n",
    "# Model 3\n",
    "!curl -k https://aramislab.paris.inria.fr/clinicadl/files/models/v0.2.0/model_exp14_splits_1.tar.gz  -o model_exp14_splits_1.tar.gz\n",
    "!tar xf model_exp14_splits_1.tar.gz\n",
    "\n",
    "# Model 4\n",
    "!curl -k https://aramislab.paris.inria.fr/clinicadl/files/models/v0.2.0/model_exp18_splits_1.tar.gz  -o model_exp18_splits_1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854a057d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Run `clinicadl classify`\n",
    "\n",
    "Running classification on a dataset is extremly simple using `clinicadl`. In\n",
    "this case, we will continue using the data preprocessed in the [previous\n",
    "notebook](./preprocessing). The models have been trained exclusively on the ADNI\n",
    "dataset, all the subjects of OASIS-1 can be used to evaluate the model (without\n",
    "risking data leakage).\n",
    "\n",
    "If you ran the previous notebook, you must have a folder called\n",
    "`OasisCaps_example` in the current directory (Otherwise uncomment the next cell\n",
    "to download a local version of the necessary folders)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42949db3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!curl -k https://aramislab.paris.inria.fr/files/data/databases/tuto/OasisCaps2.tar.gz -o OasisCaps2.tar.gz\n",
    "!tar xf OasisCaps2.tar.gz\n",
    "!curl -k https://aramislab.paris.inria.fr/files/data/databases/tuto/OasisBids.tar.gz -o OasisBids.tar.gz\n",
    "!tar xf OasisBids.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac2a331",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "In the following steps we will classify the images using the pretrained models.\n",
    "The input necessary for `clinica classify` are:\n",
    "* a CAPS directory (`OasisCaps_example`),\n",
    "* a tsv file with subjects/sessions to process, containing the diagnosis (`participants.tsv`),\n",
    "* the path to the pretrained model,\n",
    "* an output prefix for the output file.\n",
    "\n",
    "Some optional parameters includes:\n",
    "* the possibility of classifying non labeled data (without known diagnosis),\n",
    "* the option to use previously extracted patches/slices.\n",
    "\n",
    "```{warning}\n",
    "If your computer is not equiped with a GPU card add the option `-cpu` to the\n",
    "command.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff68af2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "First of all, we need to generate a valid tsv file. We use the tool `clinica iotools`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ade1eb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!clinica iotools merge-tsv OasisBids_example OasisCaps_example/participants.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dbd92b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Then, we can run the classifier for the **image-level** model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540593aa",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Execute classify on OASIS dataset\n",
    "# Model 1\n",
    "!clinicadl classify ./OasisCaps_example ./OasisCaps_example/participants.tsv ./model_exp3_splits_1 'test-Oasis'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e69ad31",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "The predictions of our classifier for the subjects of this dataset are shown next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fc6c11",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "predictions = pd.read_csv(\"./model_exp3_splits_1/fold-0/cnn_classification/best_balanced_accuracy/test-Oasis_image_level_prediction.tsv\", sep=\"\\t\")\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636407b8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Note that 0 corresponds to the **CN** class and 1 to the **AD**. It is also\n",
    "important to remember that the last two images/subjects performed badly when\n",
    "running the quality check step.\n",
    "\n",
    "`clinica classify` also produces a file containing different metrics (accuracy,\n",
    "balanced accuracy, etc.) for the current dataset. It can be displayed by running\n",
    "the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a68f80",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "metrics = pd.read_csv(\"./model_exp3_splits_1/fold-0/cnn_classification/best_balanced_accuracy/test-Oasis_image_level_metrics.tsv\", sep=\"\\t\")\n",
    "metrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e89509d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "In the same way, we can process the dataset with all the other models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bab2a88",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Model 2 3D ROI-based model\n",
    "!clinicadl classify ./OasisCaps_example ./OasisCaps_example/participants.tsv ./model_exp8_splits_1 'test-Oasis'\n",
    "\n",
    "predictions = pd.read_csv(\"./model_exp8_splits_1/fold-0/cnn_classification/best_balanced_accuracy/test-Oasis_image_level_prediction.tsv\", sep=\"\\t\")\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9fbe30",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Model 3 3D patch-level model\n",
    "!clinicadl classify ./OasisCaps_example ./OasisCaps_example/participants.tsv ./model_exp14_splits_1 'test-Oasis'\n",
    "\n",
    "predictions = pd.read_csv(\"./model_exp14_splits_1/fold-0/cnn_classification/best_balanced_accuracy/test-Oasis_image_level_prediction.tsv\", sep=\"\\t\")\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a7e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4 2D slice-level model\n",
    "!clinicadl classify ./OasisCaps_example ./OasisCaps_example/participants.tsv ./model_exp18_splits_1 'test-Oasis'\n",
    "\n",
    "predictions = pd.read_csv(\"./model_exp18_splits_1/fold-0/cnn_classification/best_balanced_accuracy/test-Oasis_image_level_prediction.tsv\", sep=\"\\t\")\n",
    "predictions.head()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
