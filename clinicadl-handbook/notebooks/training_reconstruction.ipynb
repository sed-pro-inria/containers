{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b51c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this cell if running in Google Colab\n",
    "!pip install clinicadl==1.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606ef6f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Training for reconstruction\n",
    "\n",
    "The objective of the `reconstruction` is to learn to reconstruct images given\n",
    "as input. To do so, we can use a type of artificial neural network called \n",
    "**autoencoder**.\n",
    "\n",
    "An autoencoder learns to reconstruct data given as input. It is composed of\n",
    "two parts:\n",
    "- The `encoder` which reduces the dimensionality of the input to a smaller\n",
    "feature map: the code.\n",
    "- The `decoder` which reconstructs the input based on the code.\n",
    "\n",
    "The mean squared error is used to evaluate the difference between the input\n",
    "and its reconstruction.\n",
    "\n",
    "There are many paradigms associated to autoencoders, but here we will only\n",
    "focus on a specific case that allows us to get the weights that can be\n",
    "transferred to a CNN. In ClinicaDL, the autoencoders are designed based on a\n",
    "CNN:\n",
    "- the `encoder` corresponds to the convolutional layers of the CNN,\n",
    "- the `decoder` is composed of the transposed version of the operations used\n",
    "in the encoder.\n",
    "\n",
    "After training the autoencoder, the weights in its encoder can be copied in\n",
    "the convolutional layers of a CNN to initialize it. This can improve its\n",
    "performance as the autoencoder has already learnt patterns characterizing the\n",
    "data distribution.\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"../images/autoencoder_architecture.png\" alt=\"CNN architecture\" style=\"height: 350px; margin: 10px; text-align: center;\">\n",
    "    <figcaption><i>Autoencoder derived from the previous CNN architecture</i></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d962b53",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 3D patch-level & ROI-based models\n",
    "\n",
    "The 3D patch-level models compensate the absence of 3D information in the 2D\n",
    "slice-level approach and keep some of its advantages (low memory usage and\n",
    "larger sample size). \n",
    "\n",
    "ROI-based models are similar to 3D-patch but take advantage of prior knowledge\n",
    "on Alzheimer's disease. Indeed most of the patches are not informative as they\n",
    "contain parts of the brain that are not affected by the disease. Methods based\n",
    "on regions of interest (ROI) overcome this issue by focusing on regions which\n",
    "are known to be informative: here the hippocampi. In this way, the complexity of\n",
    "the framework can be decreased as fewer inputs are used to train the networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec805cbe",
   "metadata": {},
   "source": [
    "##  3D patch-level tensor extraction with the `prepare-data` pipeline\n",
    "\n",
    "Before starting, we need to obtain files suited for the training phase. This\n",
    "pipeline prepares images generated by Clinica to be used with the PyTorch deep\n",
    "learning library [(Paszke et al.,\n",
    "2019)](https://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library).\n",
    "Four types of tensors are proposed: 3D images, 3D patches, 3D ROI or 2D\n",
    "slices.\n",
    "\n",
    "This pipeline selects the preprocessed images, extract the \"tensors\", and\n",
    "write them as output files for the entire images, for each slice, for each roi\n",
    "or for each patch.\n",
    "\n",
    "Here, as you will use slice-level, you simply need to type the following\n",
    "command line:\n",
    "\n",
    "```bash\n",
    "clinicadl prepare-data patch <caps_directory> <modality>\n",
    "```\n",
    "where:\n",
    "\n",
    "- `caps_directory` is the folder containing the results of the [`t1-linear`\n",
    "pipeline](#preprocessing:t1-linear) and the output of the present command,\n",
    "both in a CAPS hierarchy.\n",
    "- `modality` is the name of the preprocessing performed on the original\n",
    "images. It can be `t1-linear` or `pet-linear`. You can choose custom if you\n",
    "want to get a tensor from a custom filename.\n",
    "\n",
    "When using patch or slice extraction, default values were set according to\n",
    "[Wen et al., 2020](https://doi.org/10.1016/j.media.2020.101694)\n",
    "\n",
    "Output files are stored into a new folder (inside the CAPS) and follows a\n",
    "structure like this:\n",
    "\n",
    "```text\n",
    "deeplearning_prepare_data\n",
    "├── image_based\n",
    "│   └── t1_linear\n",
    "│       └── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.pt\n",
    "└── patch_based\n",
    "    └── t1_linear\n",
    "        ├── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_axis-axi_channel-rgb_patch-0_T1w.pt\n",
    "        ├── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_axis-axi_channel-rgb_patch-1_T1w.pt\n",
    "        ├── ...\n",
    "        └── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_axis-axi_channel-rgb_patch-N_T1w.pt\n",
    "```\n",
    "\n",
    "In short, there is a folder for each feature (**image, slice, roi or patch**)\n",
    "and inside the numbered tensor files with the corresponding feature. \n",
    "Files are saved with the .pt extension and contains tensors in PyTorch format.\n",
    "A JSON file is also stored in the CAPS hierarchy under the tensor_extraction\n",
    "folder:\n",
    "\n",
    "```text\n",
    "CAPS_DIRECTORY\n",
    "└── tensor_extraction\n",
    "        └── <extract_json>\n",
    "```\n",
    "These files are compulsory to run the train command. They provide all the\n",
    "details of the processing performed by the prepare-data command that will be\n",
    "necessary when reading the tensors.\n",
    "\n",
    "```{warning}\n",
    "The default behavior of the pipeline is to only extract images even if another extraction method is specified. \n",
    "However, all the options will be saved in the preprocessing JSON file and then the extraction is done when data \n",
    "s loaded during the training. If you want to save the extracted method tensors in the CAPS, you have to add \n",
    "the `--save-features` flag.\n",
    "```\n",
    "\n",
    "ClinicaDL is able to extract patches _on-the-fly_ (from one single file) when\n",
    "running training or inference tasks. The downside of this approach is that,\n",
    "depending on the size of your dataset, you have to make sure that you have\n",
    "enough memory resources in your GPU card to host the full images/tensors for\n",
    "all your data. \n",
    "\n",
    "If the memory size of the GPU card you use is too small, we suggest you to\n",
    "extract the patches and/or the slices using the proper `tensor_format` option of\n",
    "the command described above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ec982",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "(If you failed to obtain the preprocessing using the `t1-linear` pipeline,\n",
    "please uncomment the next cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e147e344",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!curl -k https://aramislab.paris.inria.fr/clinicadl/files/handbook_2023/CAPS_example.tar.gz -o oasisCaps.tar.gz\n",
    "!tar xf oasisCaps.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e585d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "To perform the feature extraction for our dataset, run the following cell:     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2b563a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!clinicadl prepare-data patch data_oasis/CAPS_example pet-linear --subjects_sessions_tsv data_adni/after_qc.tsv --extract_json pet_reconstruction --acq_label 18FFDG --suvr_reference_region cerebellumPons2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffacfba",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "At the end of this command, a new directory named `deeplearning_prepare_data` is\n",
    "created inside each subject/session of the CAPS structure. We can easily verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153ce761",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree -L 3 ./data_oasis/CAPS_example/subjects/sub-OASIS10*/ses-M00/deeplearning_prepare_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0516e3e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# We can also do the same things with region of interest. \n",
    "# ROI are 3D patches defined by masks that need to be at the root in\n",
    "# `CAPS_DIRECTORY`. \n",
    "# In the data_adni/masks folder you can find masks of the right and left\n",
    "# hippocampus that can be used to define two region of interest.\n",
    "#\n",
    "# Uncomment the next cell if you want to extract roi tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550d17bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r data/masks data_adni/CAPS_example/masks\n",
    "!clinicadl prepare-data roi data_adni/CAPS_example pet-linear --roi_list rightHippocampusBox --roi_list leftHippocampusBox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe51f64",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "These two paradigms can be divided into two different frameworks:\n",
    "- **single-network**: one network is trained on all patch locations / all\n",
    "regions.\n",
    "- **multi-network**: one network is trained per patch location / per region.\n",
    "\n",
    "For **multi-network** the sample size is smaller (equivalent to image level\n",
    "framework), however the network may be more accurate as they are specialized\n",
    "for one patch location / for one region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d17ae0d",
   "metadata": {},
   "source": [
    "As for the 2D slice-level model, the gradient updates are done based on the\n",
    "loss computed at the patch level. Final performance metrics are computed at\n",
    "the subject level by combining the outputs of the patches or the two\n",
    "hippocampi of the same subject.\n",
    "\n",
    "The default network used for reconstruction in ClinicaDL is an autoencoder\n",
    "derived from the convolutional part of CNN with 5 convolutionnal layers\n",
    "followed by 3 fully connected.  The compatible criterion loss are :\n",
    "- the mean squared error between the input and the network output\n",
    "([link](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html)),\n",
    "- the Kullback-Leibler divergence loss\n",
    "([link](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html)),\n",
    "- the mean absolute error loss\n",
    "([link](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html)), \n",
    "- a combination of a Sigmoid layer and the BCELoss\n",
    "([link](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)),\n",
    "- the Huber loss\n",
    "([link](https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html)),\n",
    "- the smooth L1 loss\n",
    "([link](https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html))\n",
    "but the default one is the mean square error.\n",
    "\n",
    "The evaluation metrics are the mean squared error (MSE) and mean absolute\n",
    "error (MAE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd04d63",
   "metadata": {},
   "source": [
    "## Before starting \n",
    "```{warning}\n",
    "If you do not have access to a GPU, training the CNN may require too much time.\n",
    "However, you can execute this notebook on Colab to run it on a GPU.\n",
    "```\n",
    "\n",
    "If you already know the models implemented in `clinicadl`, you can directly jump\n",
    "to the `train custom` to implement your own custom experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3d5f0b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyrsistent import v\n",
    "import torch\n",
    "\n",
    "# Check if a GPU is available\n",
    "print('GPU is available', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da55095",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Data used for training\n",
    "\n",
    "Because they are time-costly, the preprocessing\n",
    "steps presented in the beginning of this tutorial were only executed on a\n",
    "subset of OASIS-1, but obviously two participants are insufficient to train a\n",
    "network! To obtain more meaningful results, you should retrieve the whole <a\n",
    "href=\"https://www.oasis-brains.org/\">OASIS-1</a> dataset and run the training\n",
    "based on the labels and splits performed in the previous section.  Of course,\n",
    "you can use another dataset, but then you will have to perform again\n",
    "\"./label_extraction.ipynb\" the extraction of labels and data splits on this\n",
    "dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19faf258",
   "metadata": {},
   "source": [
    "## `clinicadl train RECONSTRUCTION` \n",
    "\n",
    "This functionality mainly relies on the PyTorch deep learning library\n",
    "[[Paszke et al., 2019](https://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library)].\n",
    "\n",
    "Different tasks can be learnt by a network: `classification`, `reconstruction`\n",
    "and `regression`, in this notebook, we focus on the `reconstruction` task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e634afbc",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "You need to execute the `clinicadl tsvtool get-labels` and `clinicadl tsvtools\n",
    "{split|kfold}`commands prior to running this task to have the correct TSV file\n",
    "organization.  Moreover, there should be a CAPS, obtained running the\n",
    "preprocessing pipeline wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e38c1ec",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Running the task\n",
    "The training task can be run with the following command line:\n",
    "```text\n",
    "clinicadl train reconstruction [OPTIONS] CAPS_DIRECTORY PREPROCESSING_JSON \\\n",
    "                TSV_DIRECTORY OUTPUT_MAPS_DIRECTORY\n",
    "```\n",
    "where mandatory arguments are:\n",
    "\n",
    "- `CAPS_DIRECTORY` (Path) is the input folder containing the neuroimaging data\n",
    "in a\n",
    "[CAPS](https://aramislab.paris.inria.fr/clinica/docs/public/latest/CAPS/Introduction/)\n",
    "hierarchy.  In case of multi-cohort training, must be a path to a TSV file.\n",
    "- `PREPROCESSING_JSON` (str) is the name of the preprocessing json file stored\n",
    "in the `CAPS_DIRECTORY` that corresponds to the `clinicadl extract` output.\n",
    "This will be used to load the correct tensor inputs with the wanted\n",
    "preprocessing.\n",
    "- `TSV_DIRECTORY` (Path) is the input folder of a TSV file tree generated by\n",
    "`clinicadl tsvtool {split|kfold}`.  In case of[multi-cohort training, must be\n",
    "a path to a TSV file.\n",
    "- `OUTPUT_MAPS_DIRECTORY` (Path) is the folder where the results are stored.\n",
    "\n",
    "The training can be configured through a [Toml\n",
    "configuration](https://clinicadl.readthedocs.io/en/latest/Train/Introduction/#configuration-file)\n",
    "file or by using the command line options. If you have a Toml configuration\n",
    "file you can use the following option to load it:\n",
    "\n",
    "- `--config_file` (Path) is the path to a Toml configuration file. This file\n",
    "contains the value for the options that you want to specify (to avoid too long\n",
    "command line).\n",
    "\n",
    "If an option is specified twice (in the configuration file and, as an option,\n",
    "in the command line) then **the values specified in the command line will\n",
    "override the values of the configuration file**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f14d0d5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "A few options depend on the regression task:\n",
    "\n",
    "- `--selection_metrics` (str) are metrics used to select networks according to\n",
    "the best validation performance.  Default: `loss`.\n",
    "- `--loss` (str) is the name of the loss used to optimize the reconstruction\n",
    "task. Must correspond to a Pytorch class. Default: `MSELoss`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c584acc4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "(If you failed to obtain the tensor extraction using the `prepare-data` \n",
    "pipeline, please uncomment the next cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e84ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -k https://aramislab.paris.inria.fr/clinicadl/files/handbook_2023/data_adni/CAPS_extracted.tar.gz -o oasisCaps_extracted.tar.gz\n",
    "!tar xf oasisCaps_extracted.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0930c301",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Please note that the purpose of this notebook is not to fully train a network \n",
    "because we don't have enough data. The objective is to understand how ClinicaDL \n",
    "works and make inferences using pretrained models in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcad7ad",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 3D-patch autoencoder pretraining\n",
    "!clinicadl train reconstruction -h\n",
    "!clinicadl train reconstruction data_adni/CAPS_example pet_reconstruction data_adni/split/4_fold/ data_adni/maps_reconstruction_3D_patch --n_splits 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6ecbd2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Transfer learning:</b> \n",
    "    <p>It is possible for these categories to train an autoencoder derived from\n",
    "    the CNN architecture. The encoder will share the same architecture as the\n",
    "    CNN until the fully-connected layers (see the <a\n",
    "    href=\"./deep_learning.md#autoencoder-pretraining\">bakground section</a> for\n",
    "    more details on autoencoders construction).</p>\n",
    "    <img src=\"../images/autoencoder.png\">\n",
    "    <p>Then the weights of the encoder will be transferred to the convolutions\n",
    "    of the CNN to initialize it before its training. This procedure is called\n",
    "    <i>autoencoder pretraining</i>.</p>\n",
    "    <p>It is also possible to transfer weights between two CNNs with the same\n",
    "    architecture.</p>\n",
    "    <p>For 3D-patch multi-CNNs specifically, it is possible to initialize each\n",
    "    CNN of a multi-CNN:\n",
    "        <ul>\n",
    "        <li> with the weights of a single-CNN,</li>\n",
    "        <li> with the weights of the corresponding CNN of a multi-CNN. </li>\n",
    "    </ul>\n",
    "    <p>Transferring weights between CNNs can be useful when performing two\n",
    "    classification tasks that are similar. This is what has been done in (<a\n",
    "    href=\"https://www.sciencedirect.com/science/article/abs/pii/S1361841520300591\">Wen\n",
    "    et al., 2020</a>): the sMCI vs pMCI classification network was initialized\n",
    "    with the weights of the AD vs CN classification network.</p>\n",
    "</div>\n",
    "\n",
    "```{warning}\n",
    "Transferring weights between tasks that are not similar enough can hurt the\n",
    "performance!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9798e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With autoencoder pretraining\n",
    "!clinicadl train classification data_adni/CAPS_example pet_reconstruction data_adni/split/4_fold data_adni/maps_classification_3D_patch_transfer --architecture Conv4_FC3 --transfer_path data_adni/maps_reconstrcution_patch --n_splits 4 --epochs 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6177bfa",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# 3D-patch multi-CNN training\n",
    "!clinicadl train classification -h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887f0ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With autoencoder pretraining\n",
    "!clinicadl train classification data_adni/CAPS_example pet_reconstruction data_adni/split/4_fold data_adni/maps_classification_transfer_AE_patch_multi --architecture Conv4_FC3 --transfer_path data_adni/maps_reconstrcution_patch --n_splits 4 --epochs 3 --multi-network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817c7e47",
   "metadata": {},
   "source": [
    "The clinicadl train command outputs a MAPS structure in which there are only\n",
    "two data groups: train and validation.  A MAPS folder contains all the\n",
    "elements obtained during the training and other post-processing procedures\n",
    "applied to a particular deep learning framework. The hierarchy is organized\n",
    "according to the fold, selection metric and data group used.\n",
    "\n",
    "An example of a MAPS structure is given below\n",
    "```text\n",
    "<maps_directory>\n",
    "├── environment.txt\n",
    "├── split-0\n",
    "│       ├── best-loss\n",
    "│       │       ├── model.pth.tar\n",
    "│       │       ├── train\n",
    "│       │       │       ├── description.log\n",
    "│       │       │       ├── train_image_level_metrics.tsv\n",
    "│       │       │       └── train_image_level_prediction.tsv\n",
    "│       │       └── validation\n",
    "│       │               ├── description.log\n",
    "│       │               ├── validation_image_level_metrics.tsv\n",
    "│       │               └── validation_image_level_prediction.tsv\n",
    "│       └── training_logs\n",
    "│               ├── tensorboard\n",
    "│               │       ├── train\n",
    "│               │       └── validation\n",
    "│               └── training.tsv\n",
    "├── groups\n",
    "│       ├── train\n",
    "│       │       ├── split-0\n",
    "│       │       │       ├── data.tsv\n",
    "│       │       │       └── maps.json\n",
    "│       │       └── split-1\n",
    "│       │               ├── data.tsv\n",
    "│       │               └── maps.json\n",
    "│       ├── train+validation.tsv\n",
    "│       └── validation\n",
    "│               ├── split-0\n",
    "│               │       ├── data.tsv\n",
    "│               │       └── maps.json\n",
    "│               └── split-1\n",
    "│                       ├── data.tsv\n",
    "│                       └── maps.json\n",
    "└── maps.json\n",
    "```\n",
    "\n",
    "You can find more information about MAPS structure on our\n",
    "[documentation](https://clinicadl.readthedocs.io/en/latest/Introduction/#maps-definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ede972",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Inference \n",
    "\n",
    "(If you failed to train the model please uncomment the next cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd881005",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -k https://aramislab.paris.inria.fr/clinicadl/files/handbook_2023/data_adni/CAPS_example.tar.gz -o adniCaps.tar.gz\n",
    "!tar xf adniCaps.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82366e92",
   "metadata": {},
   "source": [
    "The `predict` functionality performs individual prediction and metrics\n",
    "computation on a set of data using models trained with `clinicadl train` or\n",
    "`clinicadl random-search` tasks. \n",
    "It can also use any pretrained models if they are structured like a\n",
    "[MAPS](https://clinicadl.readthedocs.io/en/latest/Introduction/#maps-definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4c15cc",
   "metadata": {},
   "source": [
    "### Running the task \n",
    "This task can be run with the following command line:\n",
    "\n",
    "```bash\n",
    "  clinicadl predict [OPTIONS] INPUT_MAPS_DIRECTORY DATA_GROUP\n",
    "```\n",
    "where:\n",
    "- INPUT_MAPS_DIRECTORY (Path) is the path to the MAPS of the pretrained model.\n",
    "- DATA_GROUP (str) is the name of the data group used for the prediction.\n",
    "\n",
    "```{warning}\n",
    "For ClinicaDL, a data group is linked to a list of participants / sessions and\n",
    "a CAPS directory. When performing a prediction, interpretation or tensor\n",
    "serialization the user must give a data group. If this data group does not\n",
    "exist, the user MUST give a caps_directory and a participants_tsv. If this\n",
    "data group already exists, the user MUST not give any caps_directory or\n",
    "participants_tsv, or set overwrite to True.\n",
    "```\n",
    "\n",
    "For the reconstruction task, you can save the output tensors of a whole data\n",
    "group, associated with the tensor corresponding to their input.  This can be\n",
    "useful if you want to perform extra analyses directly on the images\n",
    "reconstructed by a trained network, or simply visualize them for a qualitative\n",
    "check.\n",
    "\n",
    "- `--save_tensor` (flag) to the reconstruction output in the MAPS in Pytorch tensor format.\n",
    "- `--save_nifti` (flag) to the reconstruction output in the MAPS in NIfTI format.\n",
    "\n",
    "If you want to add optional argument you can check the [documentation](https://clinicadl.readthedocs.io/en/latest/Predict/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b55efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!clinicadl predict -h\n",
    "!clinicadl predict data_adni/maps_reconstrcution_3D_patch 'test-adni' --caps_directory <caps_directory> --participants_tsv data_adni/split/test_baseline.tsv --save_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c3743f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!clinicadl predict data_adni/maps_classification_transfer_AE_patch 'test-adni' --caps_directory <caps_directory> --participants_tsv data_adni/split/test_baseline.tsv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d3b95",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Results are stored in the MAPS of path `model_path`, according to the\n",
    "following file system:\n",
    "```text\n",
    "model_path>\n",
    "    ├── split-0  \n",
    "    ├── ...  \n",
    "    └── split-<i>\n",
    "        └── best-<metric>\n",
    "                └── <data_group>\n",
    "                    ├── description.log\n",
    "                    ├── <prefix>_image_level_metrics.tsv\n",
    "                    ├── <prefix>_image_level_prediction.tsv\n",
    "                    ├── <prefix>_patch_level_metrics.tsv\n",
    "                    └── <prefix>_patch_level_prediction.tsv\n",
    "\n",
    "`clinica predict` produces a file containing different metrics (accuracy,\n",
    "balanced accuracy, etc.) for the current dataset. It can be displayed by\n",
    "running the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a4fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "metrics = pd.read_csv(\"data_adni/maps_reconstrcution_3D_patch/split-0/best-loss/test-adni/test-adni_patch_level_metrics.tsv\", sep=\"\\t\")\n",
    "metrics.head()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
