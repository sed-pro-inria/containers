{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d662e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this cell if running in Google Colab\n",
    "!pip install clinicadl==1.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092e3459",
   "metadata": {},
   "source": [
    "# Classification with a CNN on 2D slice.\n",
    "\n",
    "The objective of the *classification* task is to attribute a class to input\n",
    "images. A CNN takes as input an image and outputs a vector of size `C`,\n",
    "corresponding to the number of different labels existing in the dataset.  More\n",
    "precisely, this vector contains a value for each class that is often\n",
    "interpreted (after some processing) as the probability that the input image\n",
    "belongs to the corresponding class.  Then, the prediction of the CNN for a\n",
    "given image corresponds to the class with the highest probability in the\n",
    "output vector.\n",
    "\n",
    "The cross-entropy loss between the ground truth and the network output is used\n",
    "to quantify the error made by the network during the training process, which\n",
    "becomes null if the network outputs 100% probability for the true class.\n",
    "\n",
    "There are no rules regarding the architectures of CNNs, except that they\n",
    "contain convolution and activation layers.  In ClinicaDL, other layers such\n",
    "as pooling, batch normalization, dropout and fully-connected layers are also\n",
    "used.  The default CNN used for classification in ClinicaDL is `Conv5_FC3`\n",
    "which is a convolutional neural network with 5 convolution and 3\n",
    "fully-connected layer but in this notebook we will use the `resnet18`: \n",
    "\n",
    "<figure>\n",
    "  <img src=\"../images/resnet18.png\" alt=\"resnet18 architecture\" style=\"height: 300px; margin: 10px; text-align: center;\">\n",
    "    <figcaption><i>Example of a CNN architecture</i></figcaption>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2c5f25",
   "metadata": {},
   "source": [
    "##  2D slice-level tensor extraction with the `prepare-data` pipeline\n",
    "\n",
    "Before starting, we need to obtain the files suited for the training phase. This\n",
    "pipeline prepares images generated by Clinica to be used with the PyTorch deep\n",
    "learning library [(Paszke et al.,\n",
    "2019)](https://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library).\n",
    "Four types of tensors are proposed: 3D images, 3D patches, 3D ROI or 2D\n",
    "slices.\n",
    "\n",
    "The pipeline selects the preprocessed images, extract the \"tensors\", and write\n",
    "them as output files for the entire images, for each slice, for each roi or\n",
    "for each patch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99117b09",
   "metadata": {},
   "source": [
    "Here, as you will use slice-level, you simply need to type the following\n",
    "command line:\n",
    "\n",
    "```bash\n",
    "clinicadl prepare-data {image/patch/roi/slice} <caps_directory> <modality>\n",
    "```\n",
    "where:\n",
    "\n",
    "- `caps_directory` is the folder containing the results of the [`t1-linear`\n",
    "pipeline](#preprocessing:t1-linear) and the output of the present command,\n",
    "both in a CAPS hierarchy.\n",
    "- `modality` is the name of the preprocessing performed on the original\n",
    "images. It can be `t1-linear` or `pet-linear`. You can choose custom if you\n",
    "want to get a tensor from a custom filename.\n",
    "\n",
    "When using patch or slice extraction, default values were set according to\n",
    "[Wen et al., 2020](https://doi.org/10.1016/j.media.2020.101694)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110aedb9",
   "metadata": {},
   "source": [
    "Output files are stored into a new folder (inside the CAPS) and follows a\n",
    "structure like this:\n",
    "\n",
    "```text\n",
    "deeplearning_prepare_data\n",
    "├── image_based\n",
    "│   └── t1_linear\n",
    "│       └── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.pt\n",
    "├── slice_based\n",
    "│   └── t1_linear\n",
    "│       ├── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_axis-axi_channel-rgb_slice-0_T1w.pt\n",
    "│       ├── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_axis-axi_channel-rgb_slice-1_T1w.pt\n",
    "│       ├── ...\n",
    "│       └── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_axis-axi_channel-rgb_slice-N_T1w.pt\n",
    "├── patch_based\n",
    "│   └── pet-linear\n",
    "│       ├── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_axis-axi_channel-rgb_patch-0_T1w.pt\n",
    "│       ├── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_axis-axi_channel-rgb_patch-1_T1w.pt\n",
    "│       ├── ...\n",
    "│       └── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_axis-axi_channel-rgb_patch-N_T1w.pt\n",
    "└── roi_based\n",
    "    └── t1_linear\n",
    "        └── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.pt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc9c7f9",
   "metadata": {},
   "source": [
    "In short, there is a folder for each feature (**image**, **slice**, **roi** or **patch**)\n",
    "and inside the numbered tensor files with the corresponding feature. \n",
    "Files are saved with the .pt extension and contains tensors in PyTorch format.\n",
    "A JSON file is also stored in the CAPS hierarchy under the tensor_extraction\n",
    "folder:\n",
    "\n",
    "```text\n",
    "CAPS_DIRECTORY\n",
    "└── tensor_extraction\n",
    "        └── <extract_json>.json\n",
    "```\n",
    "These files are compulsory to run the train command. They provide all the\n",
    "details of the processing performed by the prepare-data command that will be\n",
    "necessary when reading the tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3173aaa8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "```{warning}\n",
    "The default behavior of the pipeline is to only extract images even if another\n",
    "extraction method is specified.  However, all the options will be saved in the\n",
    "preprocessing JSON file and then the extraction is done when data is loaded\n",
    "during the training. If you want to save the extracted method tensors in the\n",
    "CAPS, you have to add the `--save-features` flag.\n",
    "```\n",
    "\n",
    "ClinicaDL is able to extract patches/roi or slices _on-the-fly_ (from one\n",
    "single file) when running training or inference tasks. The downside of this\n",
    " approach is that, depending on the size of your dataset, you have to make \n",
    "sure that you have enough memory resources in your GPU card to host the full \n",
    "images/tensors for all your data. \n",
    "\n",
    "If the memory size of the GPU card you use is too small, we suggest that you \n",
    "extract the patches and/or the slices using the proper `tensor_format` option\n",
    "of the command described above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df372e0c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Before starting\n",
    "If you failed to obtain the preprocessing using the `t1-linear` pipeline,\n",
    "please uncomment the next cell. You can extract tensors from this CAPS but\n",
    "for the training part you will need a bigger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0e9d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -k https://aramislab.paris.inria.fr/clinicadl/files/handbook_2023/data_oasis/CAPS_example_prepared.tar.gz -o oasisCaps.tar.gz\n",
    "!tar xf oasisCaps.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976bb47a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "If you have already download the full dataset and converted it to\n",
    "CAPS, you can give the path to the dataset directory by changing\n",
    "the CAPS path. If not, just run it as written but the results will \n",
    "not be relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e269547",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "To perform the feature extraction for our dataset, run the following cell:     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1aad57",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!clinicadl prepare-data slice data_oasis/CAPS_example t1-linear --extract_json slice_classification_t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c0109a",
   "metadata": {},
   "source": [
    "At the end of this command, a new directory named `deeplearning_prepare_data`\n",
    "is created inside each subject/session of the CAPS structure. We can easily\n",
    "verify. If you failed to obtain the extracted tensors please uncomment the \n",
    "next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79563fe0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!curl -k https://aramislab.paris.inria.fr/clinicadl/files/handbook_2023/data_oasis/CAPS_extracted.tar.gz -o oasisCaps.tar.gz\n",
    "!tar xf oasisCaps.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdfa161",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree -L 3 data_oasis/CAPS_example/subjects/sub-OASIS10*/ses-M000/deeplearning_prepare_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c3683c",
   "metadata": {},
   "source": [
    "# Train your own models\n",
    "## Before starting \n",
    "```{warning}\n",
    "If you do not have access to a GPU, training the CNN may take too much\n",
    "time.  However, you can execute this notebook on Colab to run it on a GPU.\n",
    "```\n",
    "\n",
    "If you already know the models implemented in `clinicadl`, you can directly\n",
    "jump to the `train custom` to implement your own custom experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32d17c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrsistent import v\n",
    "import torch\n",
    "\n",
    "# Check if a GPU is available\n",
    "print('GPU is available: ', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e704055b",
   "metadata": {},
   "source": [
    "\n",
    "### Data used for training.\n",
    "\n",
    "Because they are time-costly, the preprocessing steps presented in the\n",
    "beginning of this tutorial were only executed on a subset of OASIS-1, but\n",
    "obviously two participants are insufficient to train a network! To obtain more\n",
    "meaningful results, you should retrieve the whole <a\n",
    "href=\"https://www.oasis-brains.org/\">OASIS-1</a> dataset and run the training\n",
    "based on the labels and splits performed in the previous section.  Of course,\n",
    "you can use another dataset, but then you will have to perform again\n",
    "\"./label_extraction.ipynb\" the extraction of labels and data splits on this\n",
    "dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef00dbc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## `train CLASSIFICATION` \n",
    "\n",
    "This functionality mainly relies on the PyTorch deep learning library\n",
    "[[Paszke et al., 2019](https://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library)].\n",
    "\n",
    "Different tasks can be learnt by a network: `classification`, `reconstruction`\n",
    "and `regression`, in this notebook, we focus on the `classification` task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12789e29",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### CNN and 2D  slice-level for classification\n",
    "\n",
    "An advantage of the 2D slice-level approach is that existing CNNs which had\n",
    "huge success for natural image classification, e.g. ResNet ([He et al.,\n",
    "2016](https://doi.org/10.1109/CVPR.2016.90)) and VGGNet ([Simonyan and\n",
    "Zisserman, 2014](https://arxiv.org/abs/1409.1556)), can be easily borrowed\n",
    "and used in a transfer learning fashion. Other advantages are the increased\n",
    "number of training samples as many slices can be extracted from a single 3D\n",
    "image, and a lower memory usage compared to using the full MR image as\n",
    "input.This paradigm can be divided into two different frameworks:\n",
    "\n",
    "- **single-CNN**: one CNN is trained on all slice locations.\n",
    "- **multi-CNN**: one CNN is trained per slice location.\n",
    "\n",
    "For **multi-CNN** the sample size is smaller (equivalent to image level\n",
    "framework), however the CNNs may be more accurate as they are specialized for\n",
    "one slice location.\n",
    "\n",
    "During training, the gradients update are done based on the loss computed at\n",
    "the slice level. Final performance metric are computed at the subject level by\n",
    "combining the outputs of the slices of the same subject."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d6bf10",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Prerequisites\n",
    "\n",
    "You need to execute `clinicadl tsvtools get-labels` and `clinicadl tsvtools\n",
    "{split|kfold}` commands prior to running this task to have the correct TSV file\n",
    "organization.  Moreover, there should be a CAPS, obtained running the\n",
    "preprocessing pipeline wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addae80a",
   "metadata": {},
   "source": [
    "### Running the task\n",
    "The training task can be run with the following command line:\n",
    "```text\n",
    "clinicadl train classification [OPTIONS] CAPS_DIRECTORY PREPROCESSING_JSON \\\n",
    "                TSV_DIRECTORY OUTPUT_MAPS_DIRECTORY\n",
    "```\n",
    "where mandatory arguments are:\n",
    "\n",
    "- `CAPS_DIRECTORY` (Path) is the input folder containing the neuroimaging data\n",
    "in a\n",
    "[CAPS](https://aramislab.paris.inria.fr/clinica/docs/public/latest/CAPS/Introduction/)\n",
    "hierarchy. In case of multi-cohort training, must be a path to a TSV file.\n",
    "- `PREPROCESSING_JSON` (str) is the name of the preprocessing json file stored\n",
    "in the `CAPS_DIRECTORY` that corresponds to the `clinicadl extract` output.\n",
    "This will be used to load the correct tensor inputs with the wanted\n",
    "preprocessing.\n",
    "- `TSV_DIRECTORY` (Path) is the input folder of a TSV file tree generated by\n",
    "`clinicadl tsvtool {split|kfold}`.\n",
    "In case of multi-cohort training, must be a path to a TSV file.\n",
    "- `OUTPUT_MAPS_DIRECTORY` (Path) is the folder where the results are stored.\n",
    "\n",
    "The training can be configured through a [TOML\n",
    "configuration](https://clinicadl.readthedocs.io/en/latest/Train/Introduction/#configuration-file)\n",
    "file or by using the command line options. If you have a TOML configuration\n",
    "file you can use the following option to load it:\n",
    "\n",
    "- `--config_file` (Path) is the path to a TOML configuration file. This file\n",
    "contains the value for the options that you want to specify (to avoid too long\n",
    "command line).\n",
    "\n",
    "If an option is specified twice (in the configuration file and, as an option,\n",
    "in the command line) then **the values specified in the command line will\n",
    "override the values of the configuration file**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8057d935",
   "metadata": {},
   "source": [
    "A few options depend on the classification task:\n",
    "- `--label` (str) is the name of the column containing the label for the\n",
    "classification task.  It must be a categorical variable, but may be of any\n",
    "type. Default: `diagnosis`.\n",
    "- `--selection_metrics` (str) are metrics used to select networks according to\n",
    "the best validation performance.  Default: `loss`.\n",
    "- `--selection_threshold` (float) is a selection threshold used for\n",
    "soft-voting. It is only taken into account if several images are extracted\n",
    "from the same original 3D image (i.e. `num_networks` > 1). Default: `0`.\n",
    "- `--loss` (str) is the name of the loss used to optimize the classification\n",
    "task.  Must correspond to a Pytorch class. Default: `CrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eeb1d7",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Users can also set themselves the `label_code` parameter, but only from the\n",
    "configuration file.  This parameter allows to choose which name as written in\n",
    "the `label` column is associated with which node value (designated by the\n",
    "corresponding integer). This way several names may be associated with the same\n",
    "node.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f5e0de",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "The default label for the classification task is `diagnosis` but as long as it\n",
    "is a categorical variable, it can be of any type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e438a1a7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "The next cell train a `resnet18` to classify 2D slices of t1-linear MRI by\n",
    "diagnosis (AD or CN). \n",
    "Please note that the purpose of this notebook is not to fully train a network\n",
    "because we don't have enough data. The objective is to understand how ClinicaDL \n",
    "works and make inferences using pretrained models in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124e9f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D-slice single-CNN training\n",
    "!clinicadl train classification -h\n",
    "!clinicadl train classification data_oasis/CAPS_example slice_classification_t1 data_oasis/split/4_fold/ data_oasis/maps_classification_2D_slice_resnet18 --n_splits 4 --architecture resnet18 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f7c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D-slice multi-CNN training\n",
    "!clinicadl train classification data_oasis/CAPS_example slice_classification_t1 data_oasis/split/4_fold/ data_oasis/maps_classification_2D_slice_multi --n_splits 4 --architecture resnet18 --multi_network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd7718",
   "metadata": {},
   "source": [
    "The clinicadl train command outputs a MAPS structure in which there are only\n",
    "two data groups: train and validation. \n",
    "A MAPS folder contains all the elements obtained during the training and other\n",
    "post-processing procedures applied to a particular deep learning framework.\n",
    "The hierarchy is organized according to the fold, selection metric and data\n",
    "group used.\n",
    "\n",
    "An example of a MAPS structure is given below\n",
    "```text\n",
    "<maps_directory>\n",
    "├── environment.txt\n",
    "├── split-0\n",
    "│       ├── best-loss\n",
    "│       │       ├── model.pth.tar\n",
    "│       │       ├── train\n",
    "│       │       │       ├── description.log\n",
    "│       │       │       ├── train_image_level_metrics.tsv\n",
    "│       │       │       └── train_image_level_prediction.tsv\n",
    "│       │       └── validation\n",
    "│       │               ├── description.log\n",
    "│       │               ├── validation_image_level_metrics.tsv\n",
    "│       │               └── validation_image_level_prediction.tsv\n",
    "│       └── training_logs\n",
    "│               ├── tensorboard\n",
    "│               │       ├── train\n",
    "│               │       └── validation\n",
    "│               └── training.tsv\n",
    "├── groups\n",
    "│       ├── train\n",
    "│       │       ├── split-0\n",
    "│       │       │       ├── data.tsv\n",
    "│       │       │       └── maps.json\n",
    "│       │       └── split-1\n",
    "│       │               ├── data.tsv\n",
    "│       │               └── maps.json\n",
    "│       ├── train+validation.tsv\n",
    "│       └── validation\n",
    "│               ├── split-0\n",
    "│               │       ├── data.tsv\n",
    "│               │       └── maps.json\n",
    "│               └── split-1\n",
    "│                       ├── data.tsv\n",
    "│                       └── maps.json\n",
    "└── maps.json\n",
    "```\n",
    "\n",
    "You can find more information about MAPS structure on our\n",
    "[documentation](https://clinicadl.readthedocs.io/en/latest/Introduction/#maps-definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92145ce0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Inference using pretrained models\n",
    "\n",
    "(If you failed to train the model please uncomment the next cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d4e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -k https://aramislab.paris.inria.fr/clinicadl/files/handbook_2023/data_oasis/maps_classification_2D_slice_multi.tar.gz -o maps_classification_2D_slice_multi.tar.gz\n",
    "!tar xf maps_classification_2D_slice_multi.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f3109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -k https://aramislab.paris.inria.fr/clinicadl/files/handbook_2023/data_oasis/maps_classification_2D_slice_resnet.tar.gz -o maps_classification_2D_slice_resnet.tar.gz\n",
    "!tar xf maps_classification_2D_slice_resnet.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4470fe2",
   "metadata": {},
   "source": [
    "If you failed to train the model, you also need to download the TSV files with \n",
    "the list of participants for each split used for the training because `clinicadl \n",
    "tsvtools split` and `clinicadl tsvtools kfold` commands randomly split data so \n",
    "you can have data leakage error (see previous [notebook](notebooks/labels_extraction.ipynb) \n",
    "for more information about data leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8856c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -k https://aramislab.paris.inria.fr/clinicadl/files/handbook_2023/data_oasis/split.tar.gz -o training_split.tar.gz\n",
    "!tar xf training_split.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471becbd",
   "metadata": {},
   "source": [
    "The `predict` functionality performs individual prediction and metrics\n",
    "computation on a set of data using models trained with `clinicadl train` or\n",
    "`clinicadl random-search` tasks. \n",
    "It can also use any pretrained models if they are structured like a\n",
    "[MAPS](https://clinicadl.readthedocs.io/en/latest/Introduction/#maps-definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ef9d3a",
   "metadata": {},
   "source": [
    "### Running the task \n",
    "This task can be run with the following command line:\n",
    "\n",
    "```bash\n",
    "  clinicadl predict [OPTIONS] INPUT_MAPS_DIRECTORY DATA_GROUP\n",
    "```\n",
    "where:\n",
    "- INPUT_MAPS_DIRECTORY (Path) is the path to the MAPS of the pretrained model.\n",
    "- DATA_GROUP (str) is the name of the data group used for the prediction.\n",
    "\n",
    "```{warning}\n",
    "For ClinicaDL, a data group is linked to a list of participants / sessions and\n",
    "a CAPS directory. When performing a prediction, interpretation or tensor\n",
    "serialization the user must give a data group. If this data group does not\n",
    "exist, the user MUST give a caps_directory and a participants_tsv. If this\n",
    "data group already exists, the user MUST not give any caps_directory or\n",
    "participants_tsv, or set overwrite to True.\n",
    "```\n",
    "\n",
    "If you want to add optional argument you can check the\n",
    "[documentation](https://clinicadl.readthedocs.io/en/latest/Predict/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc0c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "!clinicadl predict -h\n",
    "!clinicadl predict data_oasis/maps_classification_2D_slice_resnet18 'test-Oasis2' --participants_tsv ./data_oasis/split/test_baseline.tsv --caps_directory data_oasis/CAPS_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c29eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!clinicadl predict data_oasis/maps_classification_2D_slice_multi 'test-Oasis' --participants_tsv ./data_oasis/split/test_baseline.tsv --caps_directory data_oasis/CAPS_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f8fab1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Results are stored in the MAPS of path `model_path`, according to the\n",
    "following file system:\n",
    "```text\n",
    "model_path>\n",
    "    ├── split-0  \n",
    "    ├── ...  \n",
    "    └── split-<i>\n",
    "        └── best-<metric>\n",
    "                └── <data_group>\n",
    "                    ├── description.log\n",
    "                    ├── <prefix>_image_level_metrics.tsv\n",
    "                    ├── <prefix>_image_level_prediction.tsv\n",
    "                    ├── <prefix>_slice_level_metrics.tsv\n",
    "                    └── <prefix>_slice_level_prediction.tsv\n",
    "```\n",
    "\n",
    "`clinica predict` produces a file containing different metrics (accuracy,\n",
    "balanced accuracy, etc.) for the current dataset. It can be displayed by\n",
    "running the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf4700",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "metrics = pd.read_csv(\"data_oasis/maps_classification_2D_slice_resnet18/split-0/best-loss/test-Oasis/test-OASIS_slice_level_metrics.tsv\", sep=\"\\t\")\n",
    "metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ad5628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
