{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c9bec2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Uncomment this cell if running in Google Colab\n",
    "!pip install clinicadl==1.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8e4a1e",
   "metadata": {},
   "source": [
    "# Regression with 3D images\n",
    "\n",
    "The objective of the *regression* is to learn the value of a continuous\n",
    "variable given an image.\n",
    "The criterion loss is the mean squared error between the ground truth and the\n",
    "network output.\n",
    "The evaluation metrics are the mean squared error (MSE) and mean absolute\n",
    "error (MAE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4c507",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "##  3D image tensor extraction with the `prepare-data` pipeline\n",
    "\n",
    "Before starting, we need to obtain files suited for the training phase. This\n",
    "pipeline prepares images generated by Clinica to be used with the PyTorch deep\n",
    "learning library [(Paszke et al.,\n",
    "2019)](https://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library).\n",
    "Four types of tensors are proposed: 3D images, 3D patches, 3D ROI or 2D\n",
    "slices.\n",
    "\n",
    "The `prepare-data` pipeline selects the preprocessed images, extracts the\n",
    "\"tensors\", and writes them as output files for the entire images, for each\n",
    "slice, for each roi or for each patch.\n",
    "\n",
    "The following command will perform this extraction, at the image-level:\n",
    "\n",
    "```bash\n",
    "clinicadl prepare-data image <caps_directory> <modality>\n",
    "```\n",
    "where:\n",
    "\n",
    "- `caps_directory` is the folder containing the results of the [`t1-linear`\n",
    "pipeline](#preprocessing:t1-linear) and the output of the present command,\n",
    "both in a CAPS hierarchy.\n",
    "- `modality` is the name of the preprocessing performed on the original\n",
    "images. It can be `t1-linear` or `pet-linear`. You can choose custom if you\n",
    "want to get a tensor from a custom filename."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6833c5e",
   "metadata": {},
   "source": [
    "\n",
    "Output files are stored into a new folder (inside the CAPS) and follows a\n",
    "structure like this:\n",
    "\n",
    "```text\n",
    "deeplearning_prepare_data\n",
    "├── image_based\n",
    "    └── t1_linear\n",
    "        └── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.pt\n",
    "```\n",
    "\n",
    "Files are saved with the .pt extension and contains tensors in PyTorch format.\n",
    "A JSON file is also stored in the CAPS hierarchy under the tensor_extraction\n",
    "folder:\n",
    "\n",
    "```text\n",
    "CAPS_DIRECTORY\n",
    "└── tensor_extraction\n",
    "        └── <extract_json>\n",
    "```\n",
    "These files are compulsory to run the train command. They provide all the\n",
    "details of the processing performed by the prepare-data command that will be\n",
    "necessary when reading the tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af9c251",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "(If you failed to obtain the preprocessing using the `t1-linear` pipeline,\n",
    "please uncomment the next cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4b339f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d78d47",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!curl -k https://aramislab.paris.inria.fr/clinicadl/files/handbook_2023/data_adni/CAPS_example.tar.gz -o oasisCaps.tar.gz\n",
    "!tar xf oasisCaps.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c498af",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "To perform the feature extraction for our dataset, run the following cell:     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a4a427",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!clinicadl prepare-data image data_adni/CAPS_example t1-linear --extract_json image_regression_t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f869cb01",
   "metadata": {},
   "source": [
    "At the end of this command, a new directory named `deeplearning_prepare_data` is\n",
    "created inside each subject/session of the CAPS structure. If you failed to \n",
    "obtain the extracted tensors please uncomment the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd3eccf",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!curl -k https://aramislab.paris.inria.fr/clinicadl/files/handbook_2023/data_adni/CAPS_extracted.tar.gz -o oasisCaps_extracted.tar.gz\n",
    "!tar xf oasisCaps_extracted.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6946ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree -L 3 data_adni/CAPS_example/subjects/sub-ADNI005S*/ses-M00/deeplearning_prepare_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade6421e",
   "metadata": {},
   "source": [
    "ClinicaDL uses the `Conv5_FC3` convolutional network for inputs of type 3D\n",
    "image-level. This network is composed of:\n",
    "* 5 convolutional layers with kernel 3x3x3,\n",
    "* 5 max pooling layers with stride and kernel of 2 and a padding value that\n",
    "  automatically adapts to the input feature map size.\n",
    "* 3 fully-connected layers.\n",
    "\n",
    "<img src=\"../images/imageCNN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74699fd",
   "metadata": {},
   "source": [
    "## Before starting \n",
    "```{warning}\n",
    "If you do not have access to a GPU, training the CNN may require too much\n",
    "time. However, you can execute this notebook on Colab to run it on a GPU.\n",
    "```\n",
    "\n",
    "If you already know the models implemented in `clinicadl`, you can directly\n",
    "jump to the `train custom` to implement your own custom experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d355ac",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from pyrsistent import v\n",
    "import torch\n",
    "\n",
    "# Check if a GPU is available\n",
    "print('GPU is available: ', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cab2a5d",
   "metadata": {},
   "source": [
    "\n",
    "### Data used for training\n",
    "\n",
    "Because they are time-costly, the preprocessing steps presented in the\n",
    "beginning of this tutorial were only executed on a subset of OASIS-1, but\n",
    "obviously two participants are insufficient to train a network! To obtain more\n",
    "meaningful results, you should retrieve the whole <a\n",
    "href=\"https://www.oasis-brains.org/\">OASIS-1</a> dataset and run the training\n",
    "based on the labels and splits performed in the previous section.  Of course,\n",
    "you can use another dataset, but then you will have to perform again\n",
    "\"./label_extraction.ipynb\" the extraction of labels and data splits on this\n",
    "dataset.\n",
    "\n",
    "## `clinicadl train REGRESSION` \n",
    "\n",
    "This functionality mainly relies on the PyTorch deep learning library\n",
    "[[Paszke et al., 2019](https://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library)].\n",
    "\n",
    "Different tasks can be learnt by a network: `classification`, `reconstruction`\n",
    "and `regression`, in this notebook, we focus on the `regression` task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b943a0",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "You need to execute the `clinicadl tsvtools get-labels` and `clinicadl\n",
    "tsvtools {split|kfold}`commands prior to running this task to have the correct\n",
    "TSV file organization.  Moreover, there should be a CAPS, obtained running the\n",
    "preprocessing pipeline wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94be18e",
   "metadata": {},
   "source": [
    "### Running the task\n",
    "The training task can be run with the following command line:\n",
    "```bash\n",
    "clinicadl train regression [OPTIONS] CAPS_DIRECTORY PREPROCESSING_JSON \\\n",
    "                TSV_DIRECTORY OUTPUT_MAPS_DIRECTORY\n",
    "```\n",
    "where mandatory arguments are:\n",
    "\n",
    "- `CAPS_DIRECTORY` (Path) is the input folder containing the neuroimaging data\n",
    "in a\n",
    "[CAPS](https://aramislab.paris.inria.fr/clinica/docs/public/latest/CAPS/Introduction/)\n",
    "hierarchy.  In case of multi-cohort training, must be a path to a TSV file.\n",
    "- `PREPROCESSING_JSON` (str) is the name of the preprocessing json file stored\n",
    "in the `CAPS_DIRECTORY` that corresponds to the `clinicadl extract` output.\n",
    "This will be used to load the correct tensor inputs with the wanted\n",
    "preprocessing.\n",
    "- `TSV_DIRECTORY` (Path) is the input folder of a TSV file tree generated by\n",
    "`clinicadl tsvtool {split|kfold}`.  In case of[multi-cohort training, must be\n",
    "a path to a TSV file.\n",
    "- `OUTPUT_MAPS_DIRECTORY` (Path) is the folder where the results are stored.\n",
    "\n",
    "The training can be configured through a [TOML\n",
    "configuration](https://clinicadl.readthedocs.io/en/latest/Train/Introduction/#configuration-file)\n",
    "file or by using the command line options. If you have a TOML configuration\n",
    "file you can use the following option to load it:\n",
    "\n",
    "- `--config_file` (Path) is the path to a TOML configuration file. This file\n",
    "contains the value for the options that you want to specify (to avoid too long\n",
    "command line).\n",
    "\n",
    "If an option is specified twice (in the configuration file and, as an option,\n",
    "in the command line) then **the values specified in the command line will\n",
    "override the values of the configuration file**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feda896e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "A few options depend on the regression task:\n",
    "- `--label` (str) is the name of the column containing the label for the\n",
    "regression task.  It must be a continuous variable (float or int). Default:\n",
    "age.\n",
    "- `--selection_metrics` (str) are metrics used to select networks according to\n",
    "the best validation performance. Default: loss.\n",
    "- `--loss` (str) is the name of the loss used to optimize the regression task. \n",
    "Must correspond to a Pytorch class. Default: MSELoss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb04d38",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Please note that the purpose of this notebook is not to fully train a network \n",
    "because we don't have enough data. The objective is to understand how ClinicaDL \n",
    "works and make inferences using pretrained models in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030ff3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for regression on the age \n",
    "!clinicadl train regression -h\n",
    "!clinicadl train regression data_adni/CAPS_example image_regression_t1 data_adni/split/4_fold data_adni/maps_regression_image --n_splits 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3bb9c2",
   "metadata": {},
   "source": [
    "The clinicadl train command outputs a MAPS structure in which there are only two data groups: train and validation. \n",
    "A MAPS folder contains all the elements obtained during the training and other post-processing procedures applied to a \n",
    "particular deep learning framework. The hierarchy is organized according to the fold, selection metric and data group used.\n",
    "\n",
    "An example of a MAPS structure is given below\n",
    "```text\n",
    "<maps_directory>\n",
    "├── environment.txt\n",
    "├── split-0\n",
    "│       ├── best-loss\n",
    "│       │       ├── model.pth.tar\n",
    "│       │       ├── train\n",
    "│       │       │       ├── description.log\n",
    "│       │       │       ├── train_image_level_metrics.tsv\n",
    "│       │       │       └── train_image_level_prediction.tsv\n",
    "│       │       └── validation\n",
    "│       │               ├── description.log\n",
    "│       │               ├── validation_image_level_metrics.tsv\n",
    "│       │               └── validation_image_level_prediction.tsv\n",
    "│       └── training_logs\n",
    "│               ├── tensorboard\n",
    "│               │       ├── train\n",
    "│               │       └── validation\n",
    "│               └── training.tsv\n",
    "├── groups\n",
    "│       ├── train\n",
    "│       │       ├── split-0\n",
    "│       │       │       ├── data.tsv\n",
    "│       │       │       └── maps.json\n",
    "│       │       └── split-1\n",
    "│       │               ├── data.tsv\n",
    "│       │               └── maps.json\n",
    "│       ├── train+validation.tsv\n",
    "│       └── validation\n",
    "│               ├── split-0\n",
    "│               │       ├── data.tsv\n",
    "│               │       └── maps.json\n",
    "│               └── split-1\n",
    "│                       ├── data.tsv\n",
    "│                       └── maps.json\n",
    "└── maps.json\n",
    "```\n",
    "\n",
    "You can find more information about MAPS structure on our [documentation](https://clinicadl.readthedocs.io/en/latest/Introduction/#maps-definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe1475a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Inference \n",
    "\n",
    "(If you failed to train the model\n",
    "please uncomment the next cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6aae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -k https://aramislab.paris.inria.fr/clinicadl/files/handbook_2023/data_adni/maps_regression_image.tar.gz -o maps_regression_image.tar.gz\n",
    "!tar xf maps_regression_image.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcb0abc",
   "metadata": {},
   "source": [
    "The `predict` functionality performs individual prediction and metrics\n",
    "computation on a set of data using models trained with `clinicadl train` or\n",
    "`clinicadl random-search` tasks. \n",
    "It can also use any pretrained models if they are structured like a\n",
    "[MAPS](https://clinicadl.readthedocs.io/en/latest/Introduction/#maps-definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea14978",
   "metadata": {},
   "source": [
    "### Running the task \n",
    "This task can be run with the following command line:\n",
    "\n",
    "```bash\n",
    "  clinicadl predict [OPTIONS] INPUT_MAPS_DIRECTORY DATA_GROUP\n",
    "```\n",
    "where:\n",
    "- INPUT_MAPS_DIRECTORY (Path) is the path to the MAPS of the pretrained model.\n",
    "- DATA_GROUP (str) is the name of the data group used for the prediction.\n",
    "\n",
    "```{warning}\n",
    "For ClinicaDL, a data group is linked to a list of participants / sessions and\n",
    "a CAPS directory. When performing a prediction, interpretation or tensor\n",
    "serialization the user must give a data group. If this data group does not\n",
    "exist, the user MUST give a caps_directory and a participants_tsv. If this\n",
    "data group already exists, the user MUST not give any caps_directory or\n",
    "participants_tsv, or set overwrite to True.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231cf3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!clinicadl predict -h\n",
    "!clinicadl predict data_adni/maps_regression_image 'test-adni' --caps_directory <caps_directory> --participants_tsv data_adni/split/test_baseline.tsv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e3da99",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Results are stored in the MAPS of path `model_path`, according to the\n",
    "following file system:\n",
    "```text\n",
    "model_path>\n",
    "    ├── split-0  \n",
    "    ├── ...  \n",
    "    └── split-<i>\n",
    "        └── best-<metric>\n",
    "                └── <data_group>\n",
    "                    ├── description.log\n",
    "                    ├── <prefix>_image_level_metrics.tsv\n",
    "                    ├── <prefix>_image_level_prediction.tsv\n",
    "```\n",
    "\n",
    "`clinica predict` produces a file containing different metrics (accuracy,\n",
    "balanced accuracy, etc.) for the current dataset. It can be displayed by\n",
    "running the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd14ffb4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "metrics = pd.read_csv(\"data_adni/maps_regression_image/split-0/best-loss/test-Oasis/test-OASIS_slice_level_metrics.tsv\", sep=\"\\t\")\n",
    "metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aac0ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
